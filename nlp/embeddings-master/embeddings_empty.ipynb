{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "\n",
    "Natürliche Sprachen bieten uns mannigfaltige Möglichkeiten, dieselben Inhalte auf unterschiedliche Art und Weise auszudrücken. Wenn wir mit Texten arbeiten, reicht es daher in der Regel nicht aus, morphologische und syntaktische Strukturen zu betrachten, weil neben der Form und Anordnung der Wörter auch deren Bedeutung eine Rolle spielt. Als zusätzliche und wichtige Ebene kommt hier also die Semantik ins Spiel.\n",
    "\n",
    "Die Abbildung von Wörtern auf Vektoren erlaubt es uns, mit diesen zu rechnen und zum Beispiel Distanzen oder Ähnlichkeiten zu bestimmen. Embeddings haben den Vorteil, dass sie eine Dimensionsreduktion mit sich bringen und semantische Embeddings sorgen darüber hinaus dafür, dass \"verwandte\" Wörter einen geringen Abstand voneinander haben.\n",
    "\n",
    "Wir wollen uns im Folgenden zunächst mit Word2Vec beschäftigen und eine einfache Version des CBOW-Ansatzes selbst implementieren.\n",
    "\n",
    "Danach schauen wir uns Gensim als Wrapping-Bibliothek für Wordembeddingmodelle an und werfen einen Blick auf Evaluationsmethoden für Embeddings sowie die ihnen inhärenten Biase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 1: Word2Vec CBOW\n",
    "> You shall know a word by the company it keeps.\n",
    ">\n",
    "> -- <cite>J. R. Firth</cite>\n",
    "\n",
    "Auf dem oben zitierten Prinzip beruhen die beiden als Word2Vec bekannt gewordenen Modelle CBOW und Skip Gram, die 2013 von [Tomas Mikolov et al.](https://arxiv.org/abs/1301.3781) bei Google entwickelt wurden.\n",
    "Erstgenanntes Modell werden wir im Folgenden in einer einfachen Form selbst implementieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Trainingsdaten\n",
    "Die Beschaffung und Aufbereitung von Trainingsdaten ist ein wichtiger Schritt in jeder NLP-Pipeline. Heute drücken wir uns davor und greifen auf das [`text8`-Datenset](http://mattmahoney.net/dc/textdata.html) zurück, das über den [Gensim-Downloader ](https://radimrehurek.com/gensim/downloader.html) heruntergeladen werden kann. Es besteht aus einem Auszug aus der Wikipedia und ist in der Gensim-Version bereits so vorbereitet, dass es als Liste von Wortlisten, den einzelnen Wikipediaartikeln, vorliegt.\n",
    "\n",
    "Der Datensatz kann [hier](https://drive.google.com/drive/folders/1PKyQnB8Ox7QN7xdC8EjLCXZtDcpw90u3?usp=sharing) heruntergeladen werden. Ladet den Datensatz mit Pickle und wählt einen Teil der Daten (etwa 100 Artikel sollten zu Demonstrationszwecken genügen) als Testdatensatz aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "data = #TODO\n",
    "data_sample = #TODO\n",
    "del data #nicht unbedingt nötig, aber wir verbrauchen eh schon so viel Speicher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Datenvorbereitung\n",
    "Wir haben einen Datensatz, der aus Listen von Wörtern besteht. Unser Modell soll aber hinterher mit Zahlen hantieren und zwar entweder mit Wortindizes, die jedes Wort im Vokabular über einen eindeutige Nummer referenzierbar machen, oder mit One-hot-Vektoren, die als Labels dienen, mit denen der tatsächliche Output des Modells verglichen werden kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "# Beim Mapping von Wörtern zu IDs und umgekehrt sollte eine reproduzierbare Reihenfolge sichergestellt werden,\n",
    "# um das Modell später weitertrainieren und die Embedding-Matrix interpretieren zu können.\n",
    "# Diese Datenstruktur kann, aber muss nicht, als Basis dienen.\n",
    "unique_words = OrderedDict.fromkeys(# Liste alle Wörter, die in unserem data_sample vorkommen) \n",
    "\n",
    "# Mapping von Wort zu ID\n",
    "word2id = # TODO\n",
    "\n",
    "# Mapping von ID zu Wort\n",
    "id2word = # TODO\n",
    "\n",
    "# Unser Data Sample, aber mit IDs statt Wörtern \n",
    "# [['der', 'hund', 'der', 'bellt'], ['die', 'katz', 'miaut']] => [[0, 1, 0, 2], [3, 4, 5]]\n",
    "numeric_docs = [[word2id[w] for w in doc] for doc in data_sample]\n",
    "\n",
    "print('Word to id sample:', list(word2id.items())[:10], '\\n')\n",
    "print('Id to word sample:', list(id2word.items())[:10], '\\n')\n",
    "print('Documents as lists of integers:', numeric_docs[0][:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir halten einige wichtige Parameter für unser Modell fest. Die Größe des Kontextfensters sowie die Länge der Embeddingvektoren können nach Bedarf angepasst werden. Für unsere Demo wählen wir kleine Werte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = # TODO\n",
    "embedding_size = 50 # Länge der Embeddingvektoren\n",
    "window_size = 2 # Größe des Kontextfensters. Wird nach rechts und links angewandt. Gesamter Kontext hier also 4 Wörter.\n",
    "\n",
    "print('Vocabulary Size:', vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Generator\n",
    "Um nicht alle Trainingsdaten auf einmal im Speicher halten zu müssen, schreiben wir uns eine Generatorfunktion, die Batches einer frei wählbaren Größe zurückgibt. Unser Ansatz ist dennoch nicht völlig speicherschonend, weil wir uns die Datengrundlage für die Generierung dieser Batches, nämlich die Integerlisten in `numeric_docs` sehr wohl im Speicher vorhalten. Darüber sehen wir aber großzügig hinweg.\n",
    "\n",
    "Die Generatorfunktion erzeugt zwei numpy-Arrays der Länge `batch_size`, von denen das eine Listen mit Indizes der Kontexwörter enthält, die der Embedding-Layer als Eingabe erwartet, und das andere die zugehörigen One-Hot-Encodings der Mittelwörter.\n",
    "\n",
    "Fiktives und vereinfachtes Beispiel:\n",
    "<pre><code>* Fenstergröße: 1\n",
    "* Batch-Size: 2\n",
    "* Wortindizes: 'die': 0, 'ente': 1, 'lacht': 2, 'und': '3, 'quakt': 4 (und damit Vokabulargröße 5)\n",
    "* Korpus (Auszug): [['die', 'ente', 'lacht', 'und', 'quakt'], ...]\n",
    "\n",
    "\n",
    "=> Rückgabe: [[0, 2], [1, 3]], [[0, 1, 0, 0, 0], [0, 0, 1, 0, 0]]\n",
    "</code></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "\n",
    "def generate_context_word_batches(corpus, window_size, vocab_size, batch_size):\n",
    "    X = []\n",
    "    Y = []\n",
    "    current_size = 0\n",
    "    while True:\n",
    "        # TODO: Here be dragons\n",
    "        yield contexts, label_words # zwei numpy arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schneller Test\n",
    "test_batch_size = 3\n",
    "test_window_size = 2\n",
    "\n",
    "for i in range(0, 3): \n",
    "    x, y = next(generate_context_word_batches(corpus=numeric_docs, window_size=test_window_size, vocab_size=vocabulary_size, batch_size=test_batch_size))\n",
    "    for j in range(0, test_batch_size):\n",
    "        print('Context (X):', [id2word[w] for w in x[j]], '-> Target (Y):', id2word[np.argwhere(y[j])[0][0]])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Definition des Models\n",
    "Als nächstes definieren wir unser Model. Dazu verwenden wir die Sequential API von Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda\n",
    "\n",
    "#Modelldefinition\n",
    "cbow = Sequential()\n",
    "cbow.add(Embedding(input_dim=#TODO, output_dim=#TODO, input_length=#TODO))\n",
    "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)))\n",
    "cbow.add(Dense(#TODO: #Units, activation=#TODO))\n",
    "cbow.compile(loss=#TODO, optimizer='rmsprop')\n",
    "\n",
    "# Zusammenfassung\n",
    "print(cbow.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Training\n",
    "Jetzt wird es ernst: Wir trainieren unser Modell.\n",
    "Da wir eine eigene Generatorfunktion verwenden, müssen wir `steps_per_epoch` angeben. Dies ist die Anzahl der Generatoraufrufe pro Epoche. An sich ist es sinnvoll, diesen Wert auf `#samples//batch_size` zu setzen, damit das Modell pro Epoche alle Trainingsdaten sieht, aber weil wir Zeit sparen wollen, wählen wir einen geringeren Wert.\n",
    "\n",
    "Weil wir unser Modell gerne abspeichern möchten, zum Beispiel, um es später weiter zu trainieren, definieren wir eine Callback-Funktion, die das für uns übernimmt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model_checkpoint = ModelCheckpoint('embeddings.hd5', monitor='loss', verbose=1, save_best_only=True, save_weights_only=False)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "batch_size = 300\n",
    "# Sollte eigentlich eher etwas in der Art von sum([len(doc) - 2 * window_size for doc in numeric_docs])//batch_size sein, aber s. o.\n",
    "steps_per_epoch = 500 \n",
    "\n",
    "\n",
    "cbow.fit_generator(# TODO, callbacks=[#TODO])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Test\n",
    "Nachdem wir unser Modell nur sehr kurz und nur auf wenigen Daten trainiert haben, ist davon auszugehen, dass die Ergebnisse nicht optimal sind. Einen kurzen Blick wollen wir dennoch riskieren.\n",
    "\n",
    "Dazu extrahieren wir zunächst die Gewichte aus dem Embedding-Layer und schauen sie uns auszugsweise an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.models import load_model\n",
    "\n",
    "cbow = # TODO: Model laden\n",
    "embedding_weights = # TODO: Auf Embedding Layer (1. Layer) des Modells zugreifen und dort die Gewichtsmatrix extrahieren\n",
    "\n",
    "pd.DataFrame(# TODO: Was wollen wir anschauen?, index=list(id2word.values())).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da durch scharfes Hinsehen nicht unmittelbar zu erkennen ist, wie gut unsere Embeddings schon sind, machen wir stichprobenartige Tests. Dazu wählen wir einige Wörter und berechnen für deren Embeddings die Ähnlichkeit mit allen anderen Embedding-Vektoren in unserer Gewichtsmatrix. Anschließend lassen wir uns die fünf ähnlichsten Wörter ausgeben.\n",
    "Als Distanzmaß wählen wir die Kosinusdistanz, die auf der Kosinusähnlichkeit beruht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "sample_terms = ['thus', 'influence', 'bible', 'climate', 'revolution', 'unix', 'term', 'working', 'topic', 'opinion']\n",
    "sample_embeddings = # TODO\n",
    "\n",
    "# Berechne die paarweisen Distanzen zwischen Beispielwörtern und Gesamtvokabular\n",
    "distance_matrix = cosine_distances(# TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zeige die top fünf ähnlichsten Wörter zu unseren Beispielwörtern \n",
    "similar_words = {sample_term: [id2word[idx] for idx in distance_matrix[index].argsort()[1:6]] \n",
    "                   for index, sample_term in enumerate(sample_terms)}\n",
    "\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 2: Gensim als Wrapper für Word2Vec-Modelle\n",
    "Embedding-Layer begegnen einem in der Praxis in der Tat häufig. In der Regel aber nicht als Bestandteile von reinen Word Embedding-Trainingsmodellen, sondern als erster Layer für Modelle mit anderen Aufgaben. Die Embeddings werden dann entweder mit vorberechneten Werten initalisiert oder werden im Training des Modells für die Downstream-Aufgabe (Textklassifikation, Übersetzung, ...) mittrainiert.\n",
    "\n",
    "Eine komfortable Möglichkeit, eigene Word2Vec-Modelle zu trainieren, bietet [Gensim](https://radimrehurek.com/gensim/models/word2vec.html), eine Bibliothek, die für diese Modelle auch Wrapper bereitstellt, um komfortabler an die Embeddings zu kommen und mit diesen zu arbeiten.\n",
    "\n",
    "Wir wollen uns im Folgenden einen kleinen Ausschnitt der Möglichkeiten, die Gensim bietet, anschauen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Word2Vec-Model laden\n",
    "Wir werden mit vortrainierten Google-News-Embeddings arbeiten, die [hier](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing) heruntergeladen werden können. Die Vektoren haben die Länge 300.\n",
    "Ladet die Embeddings über den oben angegebenen Link herunter und verwendet gensim, um sie zu laden.\n",
    "\n",
    "**Hinweis**: Kann einen Moment dauern, bis das Dictionary, das von Wort auf Embedding abbildet, erzeugt ist. Im Zweifel ein ```limit``` angeben und nur die ersten 1,5 Mio. Embeddings laden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "embeddings = # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aus Spaß an der Freude können wir dann mal schauen, wie gut unser Modell ist bzw. ob es bestimmte von Menschen wahrgenommene Analogien bestätigt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "\n",
    "embeddings.evaluate_word_analogies(datapath(\"questions-words.txt\"), restrict_vocab=30000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Spaß mit Semantik\n",
    "Im Folgenden wollen wir uns ein bisschen mit dem Mehrwert beschäftigen, den semantische Embeddings bieten. Weitere Inspiration zum Beispiel [hier](https://www.machinelearningplus.com/nlp/gensim-tutorial/) und in der [Doku](https://radimrehurek.com/gensim/models/keyedvectors.html).\n",
    "\n",
    "Mit semantischen Vektoren lassen sich zum Beispiel folgende Fragen beantworten:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Welche Stadt ist das New York Deutschlands? (Hinweis: 'New_York' ist als Token in den Embeddings enthalten)\n",
    "print('Das deutsche New York ist: {}\\n'.format(# TODO)))\n",
    "\n",
    "# Was ist Emacs besonders ähnlich?\n",
    "print('Ähnlichste Begriffe zu \"Emacs\": {}\\n'.format(# TODO))\n",
    "\n",
    "# Und wie sieht es mit Vim aus?\n",
    "print('Ähnlichste Begriffe zu \"Vim\": {}\\n'.format(# TODO))\n",
    "\n",
    "# Wer ist eigentlich der Mozart der Naturwissenschaft?\n",
    "print('Der Mozart der Naturwissenschaft ist: {}\\n'.format(# TODO))\n",
    "\n",
    "# Welches Wort verhält sich zu 'singing' wie 'burnt' zu 'burning'?\n",
    "print('burning:burnt wie singing:{}\\n'.format(# TODO))\n",
    "\n",
    "# Sind sich Deutschland und Frankreich ähnlicher oder Deutschland und Kanada?\n",
    "print('Ähnlichkeit DE, FR: {}'.format(# TODO))\n",
    "print('Ähnlichkeit DE, CAN: {}'.format(# TODO))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bei der Interpretation der Ergebnisse ist jedoch Vorsicht geboten: Es werden zwar semantische Beziehungen abgebildet, aber die entsprechen möglicherweise nicht immer den Erwartungen.\n",
    "\n",
    "Wie ähnlich sind sich zum Beispiel \"Leben\" und \"Tod\", \"kalt\" und \"warm\", \"Norden\" und \"Süden\"?\n",
    "\n",
    "Sind die Ergebnisse wie erwartet? Warum (nicht)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Ähnlichkeiten berechnen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Embeddings sind nicht neutral, sondern spiegeln die Beziehungen wieder, die sich in den Trainingsdaten finden lassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wird Wissenschaft von Frauen oder Männern gemacht?\n",
    "print('Wissenschaft wird gemacht von: {}'.format(# TODO))\n",
    "# Sind Mörder eher Schwarze, Weiße oder Asiaten?\n",
    "print('Mörder sind: {}'.format(# TODO))\n",
    "# Was bleibt vom Mann, wenn die Intelligenz abgezogen wird?\n",
    "print('Mann ohne Intelligenz: {}'.format(# TODO))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
