{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define helper method to render inside of jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def show_state(env, step=0, info=\"\"):\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"%s | Step: %d %s\" % (env.spec.id,step,info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CartPole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 2\n",
    "\n",
    "Löse das CartPole-v0 (https://gym.openai.com/envs/CartPole-v0/) Environment mittels des bereits bekannten Q-Learning. Beachte hierbei das es sich um einen continuous Statespace handelt und dieser für den tabellelarischen Ansatz zuerst in einen discrete Space transformiert werden muss.\n",
    "\n",
    "**Hinweis:** Für die Lösung müssen nicht alle zur Verfügung stehenden Eingabefeatures genutzt werden. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Übersicht über die vom Environment bereitgestellten Features und deren Wertebereiche\n",
    "\n",
    "- x (Wagenposition) ∈ [-4.8, 4.8]\n",
    "- x’ (Wagengeschwindigkeit) ∈ [-3.4 10^38, 3.4 10^38]\n",
    "- theta (Neigungswinkel) ∈ [-0.42, 0.42]\n",
    "- theta’ (Winkelgeschwindigkeit) ∈ [-3.4 10^38, 3.4 10^38]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../setup.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def discretize(env, buckets, obs):\n",
    "    upper_bounds = [env.observation_space.high[0], 0.5, env.observation_space.high[2], math.radians(50)]\n",
    "    lower_bounds = [env.observation_space.low[0], -0.5, env.observation_space.low[2], -math.radians(50)]\n",
    "    ratios = [(obs[i] + abs(lower_bounds[i])) / (upper_bounds[i] - lower_bounds[i]) for i in range(len(obs))]\n",
    "    new_obs = [int(round((buckets[i] - 1) * ratios[i])) for i in range(len(obs))]\n",
    "    new_obs = [min(buckets[i] - 1, max(0, new_obs[i])) for i in range(len(obs))]\n",
    "    return tuple(new_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.statistics import plot\n",
    "import time\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from contextlib import suppress\n",
    "\n",
    "def interact_with_environment(env, agent, buckets=None, n_episodes=400, max_steps=200, train=True, verbose=True):      \n",
    "    statistics = []\n",
    "    \n",
    "    with suppress(KeyboardInterrupt):\n",
    "        for episode in range(n_episodes):\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            state = env.reset()\n",
    "            if buckets:\n",
    "                state = discretize(env, buckets, state) # transform state\n",
    "            episode_start_time = time.time()\n",
    "\n",
    "            for t in range(max_steps):\n",
    "                action = agent.act(state)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                if buckets:\n",
    "                    next_state = discretize(env, buckets, next_state)\n",
    "\n",
    "                if train:\n",
    "                    agent.train((state, action, next_state, reward, done))\n",
    "\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            if verbose and episode % 10 == 0:\n",
    "                speed = t / (time.time() - episode_start_time)\n",
    "                print(f'episode: {episode}/{n_episodes}, score: {total_reward}, steps: {t}, '\n",
    "                      f'e: {agent.epsilon:.3f}, speed: {speed:.2f} steps/s')\n",
    "\n",
    "            statistics.append({\n",
    "                'episode': episode,\n",
    "                'score': total_reward,\n",
    "                'steps': t\n",
    "            })\n",
    "        \n",
    "    return statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1\n",
    "\n",
    "Laut Schätzungen von Astonomen besteht das Universum aus etwa $10^{80}$ Atome. Ausgeschrieben sind das  100000000000000000000000000000000000000000000000000000000000000000000000000000000 Atome. Wie viele Zustände können in dem CartPole Environment auftreten, wenn die oben angegebenen Wertebereiche betrachtet werden? Welches Problem ergibt sich daraus für die Reinforcement Learning Aufgabe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2\n",
    "Implementiere in **q_learning_agent.py** einen Agenten, der in der Lage ist das CartPole Environment mittels Q-Learning zu lösen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from q_learning_agent import AdvancedQLearning\n",
    "\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Hyperparams\n",
    "gamma = 0.75\n",
    "epsilon = 0.001\n",
    "epsilon_min = 0.0001\n",
    "alpha = 1.0\n",
    "alpha_min = 0.001\n",
    "buckets = (1, 1, 1, 1)\n",
    "\n",
    "agent = AdvancedQLearning(action_size=action_size, buckets=buckets, gamma=gamma, \n",
    "                          epsilon=epsilon, epsilon_min=epsilon_min, \n",
    "                          alpha=alpha, alpha_min=alpha_min)\n",
    "\n",
    "statistics = interact_with_environment(env, agent, buckets, n_episodes=1000, verbose=False)\n",
    "plot(statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning mittels GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.hyperparameter_optimization import GridSearch\n",
    "from functools import partial\n",
    "\n",
    "grid_search = GridSearch(\n",
    "    # TODO: Set hyperparams for GridSearch\n",
    "    grid_params = {\n",
    "        'gamma': [0.99, 0.75],\n",
    "        'epsilon': [0.99, 0.001],\n",
    "        'alpha': [0.99, 0.01],\n",
    "        'alpha_min': [0.1, 0.01],\n",
    "        'epsilon_min': [0.1, 0.01],\n",
    "        'buckets': [(1, 1, 1, 1)]\n",
    "    },\n",
    "    fixed_params = {\n",
    "        'action_size': env.action_space.n\n",
    "    },\n",
    "    construct_env = partial(gym.make, 'CartPole-v0'),\n",
    "    construct_agent = AdvancedQLearning,\n",
    "    evaluation_func = interact_with_environment,\n",
    "    grid_params_for_evaluation_func = ['buckets'],\n",
    "    score_parameter = 'score'\n",
    ")\n",
    "grid_search.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3\n",
    "Implementiere in **dqn_agent.py** einen Agenten, der in der Lage ist das CartPole Environment mittels DQN zu lösen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dqn_agent import DQN\n",
    "\n",
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.shape[0]\n",
    "\n",
    "# Hyperparams\n",
    "annealing_steps = 1000  # not episodes!\n",
    "gamma = 0.95\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = (epsilon - epsilon_min) / annealing_steps\n",
    "alpha = 0.01\n",
    "batch_size = 32\n",
    "memory_size = 10000\n",
    "start_replay_step = 2000\n",
    "target_model_update_interval = 1000\n",
    "\n",
    "agent = DQN(action_size=action_size, state_size=state_size, gamma=gamma, \n",
    "            epsilon=epsilon, epsilon_decay=epsilon_decay, epsilon_min=epsilon_min, \n",
    "            alpha=alpha, batch_size=batch_size, memory_size=memory_size,\n",
    "            start_replay_step=start_replay_step, \n",
    "            target_model_update_interval=target_model_update_interval)\n",
    "\n",
    "statistics = interact_with_environment(env, agent, verbose=True)\n",
    "env.close()\n",
    "plot(statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import Monitor\n",
    "from lib.statistics import html_video_embedding\n",
    "\n",
    "# capture every episode and clean 'video' folder before each run\n",
    "env = gym.make('CartPole-v0')\n",
    "env = Monitor(env, './video', video_callable=lambda episode_id: True, force=True, uid='id')\n",
    "statistics = interact_with_environment(env, agent, n_episodes=10, train=False, verbose=False)\n",
    "env.close()\n",
    "\n",
    "plot(statistics, y_limits=(0,200))\n",
    "html_video_embedding(statistics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
