{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "colab": {
      "name": "04_1_ModelNet10_PointNet_Classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HkCy0z3-mHS",
        "colab_type": "text"
      },
      "source": [
        "# PointNet Transfer Learning on subset of ModelNet10\n",
        "\n",
        "\n",
        "The ModelNet10 dataset contains 4899 3D Meshes from 10 different classes.\n",
        "\n",
        "The classes are:\n",
        "\n",
        "* bed\n",
        "* monitor\n",
        "* desk\n",
        "* chair\n",
        "* dresser\n",
        "* toilet\n",
        "* sofa\n",
        "* table\n",
        "* night stand\n",
        "* bathttub \n",
        "\n",
        "We provide you a pretrained version of PointNet. It was trained on 6 of the above 10 classes. Your task is to implement the PointNet Model and train it to classify the other 4 classes using transfer learning.\n",
        "\n",
        "The classes to be learnt are:\n",
        "\n",
        "* desk\n",
        "* chair\n",
        "* toilet \n",
        "* table\n",
        "\n",
        "\n",
        "\n",
        "Tasks:\n",
        "* implement the TODOs\n",
        "* transfer learn the PointNet to achieve an Accuracy > 90%\n",
        "Help:\n",
        "* use the PointNet Paper [clickedy](http://stanford.edu/~rqi/pointnet/)\n",
        "* use the Keras API Documentation [clickedy](https://www.keras.io/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAAoBY90CCyW",
        "colab_type": "code",
        "outputId": "4ca9a66c-f3a4-477e-e478-54dc237264fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/data')\n",
        "\n",
        "# open a file, where you stored the pickled data\n",
        "#file = open('data/text8', 'rb')\n",
        "# file = open('/data/My Drive/Colab Notebooks/data/text8', 'rb')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-YG9ZSs-mHU",
        "colab_type": "code",
        "outputId": "45e5dbfc-681e-4f81-8bcb-48e19f8ba3ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 870
        }
      },
      "source": [
        "# We need Open3D for the preprocessing\n",
        "%pip install keras\n",
        "%pip install open3d"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.8.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.3.2)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.17.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.12.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.8)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.0)\n",
            "Collecting open3d\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/75/c029e5cf8b7fd491d71f29cf31b9bf7f4a66dba130314cf5cebc367ffa56/open3d-0.8.0.0-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from open3d) (1.17.4)\n",
            "Requirement already satisfied: widgetsnbextension in /usr/local/lib/python3.6/dist-packages (from open3d) (3.5.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from open3d) (5.2.2)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from open3d) (7.5.1)\n",
            "Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.6/dist-packages (from notebook->open3d) (4.5.3)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from notebook->open3d) (0.2.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from notebook->open3d) (2.10.3)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from notebook->open3d) (4.6.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from notebook->open3d) (5.3.4)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from notebook->open3d) (4.6.1)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.6/dist-packages (from notebook->open3d) (4.4.0)\n",
            "Requirement already satisfied: traitlets>=4.2.1 in /usr/local/lib/python3.6/dist-packages (from notebook->open3d) (4.3.3)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from notebook->open3d) (5.6.1)\n",
            "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook->open3d) (0.8.3)\n",
            "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.6/dist-packages (from ipywidgets->open3d) (5.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->notebook->open3d) (1.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->notebook->open3d) (2.6.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->notebook->open3d) (17.0.0)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat->notebook->open3d) (2.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2.1->notebook->open3d) (1.12.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2.1->notebook->open3d) (4.4.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->open3d) (3.1.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->open3d) (0.4.4)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->open3d) (0.3)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->open3d) (0.8.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->open3d) (1.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->open3d) (2.1.3)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->open3d) (0.6.0)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.3.3; sys_platform != \"win32\"->notebook->open3d) (0.6.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->open3d) (0.7.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->open3d) (41.6.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->open3d) (0.8.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->open3d) (1.0.18)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->open3d) (4.7.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->notebook->open3d) (0.5.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->open3d) (0.1.7)\n",
            "Installing collected packages: open3d\n",
            "Successfully installed open3d-0.8.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzogT0OtU6VX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cy5Pipxu-mHZ",
        "colab_type": "code",
        "outputId": "9a801dac-68e6-4549-a5e8-15ea60519929",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "import open3d as o3d\n",
        "import os\n",
        "\n",
        "from keras.utils import Sequence, to_categorical\n",
        "import keras\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\" \n",
        "\n",
        "%load_ext autoreload\n",
        "%matplotlib inline\n",
        "import numpy as np"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqVa7mrZ-mHc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%sh\n",
        "# download ModelNet10\n",
        "wget -q http://3dvision.princeton.edu/projects/2014/3DShapeNets/ModelNet10.zip\n",
        "# unpack\n",
        "unzip -oq ModelNet10.zip\n",
        "# remove Archive\n",
        "rm ModelNet10.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFuJumxA-mHe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "KEEP = [\"chair\", \"desk\", \"toilet\", \"table\"]\n",
        "classes = [f for f in os.scandir(\"ModelNet10/\") if os.path.isdir(f)]\n",
        "\n",
        "import shutil\n",
        "\n",
        "for c in classes:\n",
        "    if c.name not in KEEP:\n",
        "        shutil.rmtree(c.path, ignore_errors=False, onerror=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdBoT3hX-mHg",
        "colab_type": "text"
      },
      "source": [
        "##  Part One: Data Provider\n",
        "\n",
        "We use the Keras Sequence API to construct a data provider, which feeds our model during training and validation. Fill in all the ToDos to make it work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ejt6YlHv-mHh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ModelNetProvider(Sequence):\n",
        "    \"\"\"\n",
        "    Lazily load point clouds and annotations from filesystem and prepare it for model training.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset, batch_size, n_classes, sample_size):\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.n_classes = n_classes\n",
        "        self.sample_size = sample_size\n",
        "\n",
        "        # indices of samples used for shuffling\n",
        "        self.indices = np.arange(len(dataset))\n",
        "\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.sample_size // self.batch_size # ToDo: Define the length of the generator (Hint: Number of Steps in one Epoch)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Generate one batch of data.\"\"\"\n",
        "        #index = index * self.batch_size\n",
        "        #batch_indices = np.arange(index, index + self.batch_size) # ToDo: select the indices for the current batch, starting at `index`\n",
        "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "        batch_samples = self.dataset.iloc[batch_indices]\n",
        "\n",
        "        return self.__generate_data(batch_samples)\n",
        "\n",
        "    def __generate_data(self, batch_samples):\n",
        "        X = []\n",
        "        y = []\n",
        "        for i, row in batch_samples.iterrows():\n",
        "            mesh = o3d.io.read_triangle_mesh((row[\"path\"]))\n",
        "            pcd = mesh.sample_points_uniformly(number_of_points=self.sample_size)\n",
        "            points = np.asarray(pcd.points)\n",
        "            mean_points = points.mean(axis=0)\n",
        "            centered_points = np.asarray([point - mean_points for point in points]) # ToDo center the points to origin\n",
        "\n",
        "            max_points = np.abs(points).max()\n",
        "            normalized_points = centered_points / max_points # ToDo normalize the centered points\n",
        "            X.append(normalized_points)\n",
        "            y.append(row[\"class\"])\n",
        "\n",
        "        return self.rotate_point_clouds(np.array(X)), to_categorical(np.array(y), num_classes=self.n_classes)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Shuffle training data, so batches are in different order\"\"\"\n",
        "        np.random.shuffle(self.indices)\n",
        "        \n",
        "        \n",
        "    def rotate_point_clouds(self, batch, rotation_angle_range=(-np.pi/8, np.pi/8)):\n",
        "        \"\"\"Rotate point cloud around y-axis (=up) by random angle\"\"\"\n",
        "        for b, pc in enumerate(batch):\n",
        "            phi = np.random.uniform(*rotation_angle_range)\n",
        "            c, s = np.cos(phi), np.sin(phi)\n",
        "            R = np.matrix([[c, 0, s],\n",
        "                           [0, 1, 0],\n",
        "                           [-s, 0, c]])\n",
        "            batch[b, :, :3] = np.dot(pc[:, :3], R)\n",
        "        return batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzLVchzC-mHm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_dataset(data_directory, file_extension=\".off\"):\n",
        "    \"\"\"\n",
        "    Loads an index to all files and structures them.\n",
        "    :param data_directory: directory containing the data files\n",
        "    :param file_extension: extension of the data files\n",
        "    :return: pandas dataframe containing an index to all files and a label index, \n",
        "        mapping numerical label representations to label names.\n",
        "    \"\"\"\n",
        "    files = [\n",
        "        os.path.join(r, f)  \n",
        "            for r, d, fs in os.walk(data_directory) \n",
        "            for f in fs if f.endswith(file_extension)\n",
        "        ]\n",
        "    \n",
        "    dataframe = pd.DataFrame({\n",
        "        \"path\": files,\n",
        "        \"class\": pd.Categorical([f.rsplit(\"/\", 3)[1] for f in files]),\n",
        "        \"is_train\": [\"train\" in f for f in files]\n",
        "    })\n",
        "    \n",
        "    factorization = dataframe[\"class\"].factorize()\n",
        "    dataframe[\"class\"] = factorization[0]\n",
        "    \n",
        "    return dataframe, factorization[1]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNpPlK-y-mHj",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "5ab1cd5f-dc8b-494a-e32b-1cd9aa5dfb52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"df, l_idx = initialize_dataset(\"ModelNet10/\")\n",
        "batch_indices = np.arange(0, 3) # ToDo: select the indices for the current batch, starting at `index`\n",
        "batch_samples = df.iloc[[5]]\n",
        "print(batch_indices)\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "for i, row in batch_samples.iterrows():\n",
        "    mesh = o3d.io.read_triangle_mesh((row[\"path\"]))\n",
        "    pcd = mesh.sample_points_uniformly(number_of_points=500)\n",
        "    points = np.asarray(pcd.points)\n",
        "    #print(points.mean(axis=0))\n",
        "    #print(points)\n",
        "    mean_points = points.mean(axis=0)\n",
        "    centered_points = np.asarray([point - mean_points for point in points]) # ToDo center the points to origin\n",
        "    \n",
        "    max_points = np.abs(points).max(axis=0)\n",
        "    normalized_points = centered_points / max_points # ToDo normalize the centered points\n",
        "    X.append(normalized_points)\n",
        "    y.append(row[\"class\"])\n",
        "\n",
        "print(normalized_points)\n",
        "#[-0.01174359  2.37781578 -0.59867143]\n",
        "#[-0.07872658  1.99953358 -1.0851834 ]\"\"\""
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'df, l_idx = initialize_dataset(\"ModelNet10/\")\\nbatch_indices = np.arange(0, 3) # ToDo: select the indices for the current batch, starting at `index`\\nbatch_samples = df.iloc[[5]]\\nprint(batch_indices)\\n\\nX = []\\ny = []\\nfor i, row in batch_samples.iterrows():\\n    mesh = o3d.io.read_triangle_mesh((row[\"path\"]))\\n    pcd = mesh.sample_points_uniformly(number_of_points=500)\\n    points = np.asarray(pcd.points)\\n    #print(points.mean(axis=0))\\n    #print(points)\\n    mean_points = points.mean(axis=0)\\n    centered_points = np.asarray([point - mean_points for point in points]) # ToDo center the points to origin\\n    \\n    max_points = np.abs(points).max(axis=0)\\n    normalized_points = centered_points / max_points # ToDo normalize the centered points\\n    X.append(normalized_points)\\n    y.append(row[\"class\"])\\n\\nprint(normalized_points)\\n#[-0.01174359  2.37781578 -0.59867143]\\n#[-0.07872658  1.99953358 -1.0851834 ]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3xQhz7pb6Ql",
        "colab_type": "code",
        "outputId": "e77e5160-279d-48b6-98cd-638b1da3b163",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
        "import numpy as np\n",
        "\n",
        "X = []\n",
        "Y = []\n",
        "Z = []\n",
        "\n",
        "for row in points:\n",
        "  X.append(row[0])\n",
        "  Y.append(row[1])\n",
        "  Z.append(row[2])\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.gca(projection='3d')\n",
        "surf = ax.scatter(X, Y, Z)\n",
        "#surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm,\n",
        "#                       linewidth=0, antialiased=False)\"\"\""
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"from mpl_toolkits.mplot3d import Axes3D\\nimport matplotlib.pyplot as plt\\nfrom matplotlib import cm\\nfrom matplotlib.ticker import LinearLocator, FormatStrFormatter\\nimport numpy as np\\n\\nX = []\\nY = []\\nZ = []\\n\\nfor row in points:\\n  X.append(row[0])\\n  Y.append(row[1])\\n  Z.append(row[2])\\n\\nfig = plt.figure()\\nax = fig.gca(projection='3d')\\nsurf = ax.scatter(X, Y, Z)\\n#surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm,\\n#                       linewidth=0, antialiased=False)\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEzA-iAM-mHo",
        "colab_type": "text"
      },
      "source": [
        "## Part Two: PointNet Architecture\n",
        "\n",
        "Implement all the missing code pieces to complete the whole architecture. Follow the description in the paper or use the image below. If your model definition is right, you should be able to load our pretrained weigths without an error. \n",
        "\n",
        "We provided the definition of the T-Net, so you don't need to implement it on your own. If you are interested in the implementation details, just have a look at the pointnet_utils.py file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnSXcCD5-mHp",
        "colab_type": "text"
      },
      "source": [
        "![PointNet Architecture](http://stanford.edu/~rqi/pointnet/images/pointnet.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWn9DIgU-mHp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        "from keras.layers import Input, Dropout, Dense, Dot, Lambda, \\\n",
        "    Reshape, concatenate, GlobalMaxPooling1D, BatchNormalization, \\\n",
        "    Activation, Conv1D, Multiply\n",
        "from keras.models import Model\n",
        "\n",
        "\n",
        "from pointnet_utils import transform_net\n",
        "\n",
        "def conv1d_bn(x, num_filters, kernel_size, padding='same', strides=1,\n",
        "              use_bias=False, scope=None, activation='relu'):\n",
        "    \"\"\"\n",
        "    Utility function to apply Convolution + Batch Normalization.\n",
        "    \"\"\"\n",
        "    with K.name_scope(scope):\n",
        "        input_shape = x.get_shape().as_list()[-2:]\n",
        "        x = Conv1D(input_shape=input_shape, filters=num_filters, kernel_size=kernel_size, padding=padding, strides=strides,\n",
        "              use_bias=use_bias, activation=activation)(x) # ToDo define the Convolutional Part of this Layer\n",
        "        x = BatchNormalization()(x) # ToDo add BatchNormalization\n",
        "        x = Activation(activation)(x) # ToDo add activation function\n",
        "    return x\n",
        "\n",
        "\n",
        "def dense_bn(x, units, use_bias=True, scope=None, activation=None):\n",
        "    \"\"\"\n",
        "    Utility function to apply Dense + Batch Normalization.\n",
        "    \"\"\"\n",
        "    with K.name_scope(scope):\n",
        "        x = Dense(units, use_bias=use_bias, activation=activation)(x)# ToDo Add Dense Part of this Layer\n",
        "        x = BatchNormalization()(x) # ToDo add BatchNormalization\n",
        "        x = Activation(activation)(x) # ToDo add activation function\n",
        "    return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YbxoIOn-mHs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pointnet(input_shape, classes):\n",
        "    \"\"\"\n",
        "    PointNet Model definition for classification.\n",
        "    :param input_shape: The point cloud shape\n",
        "    :param classes: Number of classes in output.\n",
        "    :return: PointNet Model for classification with `classes` classes.\n",
        "    \"\"\"\n",
        "    # Generate input tensor\n",
        "    inputs = Input(shape=input_shape)# ToDo define Input-Layer\n",
        "\n",
        "    # Obtain spatial point transform from inputs and convert inputs\n",
        "    ptransform = transform_net(inputs, dense_bn, conv1d_bn, scope='transform_net1', regularize=False)\n",
        "    point_cloud_transformed = Dot(axes = (2, 1))([inputs, ptransform]) # ToDo Perform the matrix multiply between ptransform and inputs\n",
        "\n",
        "    # First block of convolutions\n",
        "    net = conv1d_bn(point_cloud_transformed, num_filters=64, kernel_size=(1), scope='First_block', use_bias=True) # ToDo, define first Conv-Layer\n",
        "    net = conv1d_bn(net, num_filters=64, kernel_size=(1), scope='First_block', use_bias=True)# ToDo, define second Conv-Layer\n",
        "\n",
        "    # Obtain feature transform and apply it to the network\n",
        "    ftransform = transform_net(net, dense_bn, conv1d_bn, scope='transform_net2', regularize=True)\n",
        "    net_transformed = Dot(axes = (2, 1))([net, ftransform])# ToDo Perform the matrix multiply between ftransform and net\n",
        "\n",
        "    # Second block of convolutions\n",
        "    net = conv1d_bn(net_transformed, num_filters=64, kernel_size=(1), scope='Second_block', use_bias=True) # ToDo, define third Conv-Layer\n",
        "    net = conv1d_bn(net, num_filters=128, kernel_size=(1), scope='Second_block', use_bias=True) # ToDo, define fourth Conv-Layer\n",
        "    net = conv1d_bn(net, num_filters=1024, kernel_size=(1), scope='Second_block', use_bias=True) # ToDo, define fifth Conv-Layer\n",
        "\n",
        "    # add Maxpooling \n",
        "    net = GlobalMaxPooling1D()(net)# Todo define MaxPool Layer\n",
        "\n",
        "    # Top layers\n",
        "    net = dense_bn(net, 512, scope='embedding_layer')# ToDo, define first Dense-Layer\n",
        "    net = Dropout(0.2)(net) # ToDo, define first Dropout-Layer\n",
        "    net = dense_bn(net, 256, scope='embedding_layer')# ToDo, define second Dense-Layer\n",
        "    net = Dropout(0.2)(net)# ToDo, define second Dropout-Layer\n",
        "    #net = dense_bn(net, classes, scope='embedding_layer', activation = 'softmax')# ToDo, define third Dense-Layer\n",
        "    net = Dense(units=classes, use_bias=True, activation='softmax')(net)\n",
        "\n",
        "    model = Model(inputs, net, name='pointnet')\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdxPaxJi-mHu",
        "colab_type": "text"
      },
      "source": [
        "## Part Three: Transfer Learning of the PointNet\n",
        "\n",
        "Load the pretrained weigths for PointNet and change the output to four classes. Define all Hyperparameters and implement the train/test and your own learning rate scheduling algorithm. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "hgXsHbFw-mHv",
        "colab_type": "code",
        "outputId": "4870d20b-f885-4516-c264-fbf2f130cf33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "pretrained_model = pointnet((None, 3), 6)\n",
        "# pretrained_model.summary()\n",
        "pretrained_model.load_weights(\"/data/My Drive/Colab Notebooks/pretrained-modelnet.h5\")\n",
        "\n",
        "# print(pretrained_model.layers[-1].weights)\n",
        "# print(pretrained_model.inputs)\n",
        "# print(pretrained_model.outputs)\n",
        "\n",
        "# We need to replace the last layer, because it was trained to predict 6 classes instead of 4.\n",
        "embedding_layer = pretrained_model.layers[-2] # ToDo find \"embedding layer\" (The layer before the old softmax layer)\n",
        "outputs = Dense(units=4, use_bias=True, activation='softmax')(embedding_layer.output) # ToDo Define new Output Layer\n",
        "new_model = Model(inputs=pretrained_model.inputs, outputs=outputs)\n",
        "\n",
        "new_model.summary()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, None, 3)      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, None, 64)     256         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, None, 64)     256         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, None, 64)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, None, 128)    8320        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, None, 128)    512         conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, None, 128)    0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, None, 1024)   132096      activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, None, 1024)   4096        conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, None, 1024)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_1 (GlobalM (None, 1024)         0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 512)          524800      global_max_pooling1d_1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 512)          2048        dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 512)          0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 256)          131328      activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 256)          1024        dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 256)          0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 9)            2313        activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "reshape_1 (Reshape)             (None, 3, 3)         0           dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dot_1 (Dot)                     (None, None, 3)      0           input_1[0][0]                    \n",
            "                                                                 reshape_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_4 (Conv1D)               (None, None, 64)     256         dot_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, None, 64)     256         conv1d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, None, 64)     0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_5 (Conv1D)               (None, None, 64)     4160        activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, None, 64)     256         conv1d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, None, 64)     0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_6 (Conv1D)               (None, None, 64)     4160        activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, None, 64)     256         conv1d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, None, 64)     0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_7 (Conv1D)               (None, None, 128)    8320        activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, None, 128)    512         conv1d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, None, 128)    0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_8 (Conv1D)               (None, None, 1024)   132096      activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, None, 1024)   4096        conv1d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, None, 1024)   0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_2 (GlobalM (None, 1024)         0           activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 512)          524800      global_max_pooling1d_2[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 512)          2048        dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 512)          0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 256)          131328      activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 256)          1024        dense_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 256)          0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 4096)         1052672     activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "reshape_2 (Reshape)             (None, 64, 64)       0           dense_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dot_2 (Dot)                     (None, None, 64)     0           activation_7[0][0]               \n",
            "                                                                 reshape_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_9 (Conv1D)               (None, None, 64)     4160        dot_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, None, 64)     256         conv1d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, None, 64)     0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_10 (Conv1D)              (None, None, 128)    8320        activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, None, 128)    512         conv1d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, None, 128)    0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_11 (Conv1D)              (None, None, 1024)   132096      activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, None, 1024)   4096        conv1d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, None, 1024)   0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_3 (GlobalM (None, 1024)         0           activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 512)          524800      global_max_pooling1d_3[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 512)          2048        dense_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 512)          0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 512)          0           activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 256)          131328      dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 256)          1024        dense_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 256)          0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 256)          0           activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 4)            1028        dropout_2[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 3,482,957\n",
            "Trainable params: 3,470,797\n",
            "Non-trainable params: 12,160\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlg0WkA8TESz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for layer in new_model.layers[:-1]:\n",
        "#  layer.trainable = False\n",
        "\n",
        "#new_model.layers[-9].trainable = True\n",
        "\n",
        "#for layer in new_model.layers:\n",
        "#   print(layer.name, layer.trainable)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOeEbFZG-mHx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Define Hyperparameters\n",
        "SAMPLE_SIZE = 500\n",
        "EPOCHS = 25\n",
        "TRAIN_TEST_SPLIT = 0.8\n",
        "BATCH_SIZE = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdETiQNE-mHz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "def random_split(samples, atFraction):\n",
        "    \"\"\"\n",
        "    Perform the train/test split.\n",
        "    \"\"\"\n",
        "    print(\"atFraction = \", atFraction)\n",
        "    # ToDo perform the train/test split.\n",
        "    #samples = shuffle(samples)\n",
        "    return train_test_split(samples, train_size=atFraction, shuffle=True)\n",
        "\n",
        "\n",
        "def reduce_learning_rate(epoch, currentLearningRate):\n",
        "    \"\"\"\n",
        "    Implements adaptive learning rate scheduling.\n",
        "    \n",
        "    drop = 0.5\n",
        "    epochs_drop = 2.0\n",
        "    lrate = currentLearningRate * math.pow(drop,math.floor((1+epoch)/epochs_drop))\n",
        "    return lrate\n",
        "    \n",
        "    \"\"\"\n",
        "    decay_rate = 0.1\n",
        "    decay_step = 5\n",
        "    if epoch % decay_step == 0 and epoch:\n",
        "        return currentLearningRate * decay_rate #/ (epoch + 1)#\n",
        "    return currentLearningRate\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIMx3tEP-mH1",
        "colab_type": "code",
        "outputId": "85d5f0df-f06b-4224-dc49-f8e453bb3592",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "df, l_idx = initialize_dataset(\"ModelNet10/\")\n",
        "\n",
        "train_samples, validation_samples = random_split(df[df[\"is_train\"]], TRAIN_TEST_SPLIT)\n",
        "test_samples = df[~df[\"is_train\"]]\n",
        "\n",
        "print(df.head())\n",
        "\n",
        "print(\"Number of training samples: \", len(train_samples))\n",
        "print(\"Number of validation samples: \", len(validation_samples))\n",
        "print(\"Number of hold out test samples: \", len(test_samples))\n",
        "\n",
        "generator_training = ModelNetProvider(train_samples, batch_size=BATCH_SIZE, n_classes=4, sample_size=SAMPLE_SIZE)\n",
        "generator_validation = ModelNetProvider(validation_samples, batch_size=BATCH_SIZE, n_classes=4, sample_size=len(validation_samples))\n",
        "\n",
        "new_model.compile(optimizer=\"adam\", loss='categorical_crossentropy',\n",
        "              metrics=[\"categorical_accuracy\"])\n",
        "\n",
        "pathname = '/data/My Drive/Colab Notebooks/'\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.LearningRateScheduler(reduce_learning_rate, verbose=1),\n",
        "    keras.callbacks.TensorBoard(\"logs/\", batch_size=BATCH_SIZE),\n",
        "    keras.callbacks.ModelCheckpoint(pathname + \"checkpoints/weights.{epoch:03d}-{val_loss:.2f}.h5\", save_weights_only=True, save_best_only=True),\n",
        "    #tbCallBack\n",
        "]"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "atFraction =  0.8\n",
            "                                 path  class  is_train\n",
            "0  ModelNet10/desk/test/desk_0270.off      0     False\n",
            "1  ModelNet10/desk/test/desk_0242.off      0     False\n",
            "2  ModelNet10/desk/test/desk_0259.off      0     False\n",
            "3  ModelNet10/desk/test/desk_0277.off      0     False\n",
            "4  ModelNet10/desk/test/desk_0261.off      0     False\n",
            "Number of training samples:  1460\n",
            "Number of validation samples:  365\n",
            "Number of hold out test samples:  386\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWhijiq7-mH4",
        "colab_type": "code",
        "outputId": "0fb0d05d-f143-41ff-e506-8fc163d22dad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# train model\n",
        "new_model.fit_generator(generator=generator_training, epochs=EPOCHS, callbacks=callbacks, \n",
        "                    validation_data=generator_validation, use_multiprocessing=False, shuffle=True)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "50/50 [==============================] - 69s 1s/step - loss: 0.9035 - categorical_accuracy: 0.7420 - val_loss: 184.8860 - val_categorical_accuracy: 0.6500\n",
            "Epoch 2/25\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "50/50 [==============================] - 61s 1s/step - loss: 0.9833 - categorical_accuracy: 0.6960 - val_loss: 300355.3147 - val_categorical_accuracy: 0.6472\n",
            "Epoch 3/25\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "50/50 [==============================] - 61s 1s/step - loss: 0.8661 - categorical_accuracy: 0.6820 - val_loss: 1334.3152 - val_categorical_accuracy: 0.5333\n",
            "Epoch 4/25\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "50/50 [==============================] - 62s 1s/step - loss: 0.7927 - categorical_accuracy: 0.7160 - val_loss: 8.2733 - val_categorical_accuracy: 0.7750\n",
            "Epoch 5/25\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "50/50 [==============================] - 62s 1s/step - loss: 0.6314 - categorical_accuracy: 0.7900 - val_loss: 33.5938 - val_categorical_accuracy: 0.7778\n",
            "Epoch 6/25\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.00010000000474974513.\n",
            "50/50 [==============================] - 62s 1s/step - loss: 0.6083 - categorical_accuracy: 0.8060 - val_loss: 3.1466 - val_categorical_accuracy: 0.8472\n",
            "Epoch 7/25\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.00010000000474974513.\n",
            "50/50 [==============================] - 59s 1s/step - loss: 0.5397 - categorical_accuracy: 0.8140 - val_loss: 0.9263 - val_categorical_accuracy: 0.8750\n",
            "Epoch 8/25\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.00010000000474974513.\n",
            "50/50 [==============================] - 58s 1s/step - loss: 0.4632 - categorical_accuracy: 0.8640 - val_loss: 0.8127 - val_categorical_accuracy: 0.8639\n",
            "Epoch 9/25\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.00010000000474974513.\n",
            "50/50 [==============================] - 59s 1s/step - loss: 0.4534 - categorical_accuracy: 0.8520 - val_loss: 0.7064 - val_categorical_accuracy: 0.8750\n",
            "Epoch 10/25\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.00010000000474974513.\n",
            "50/50 [==============================] - 57s 1s/step - loss: 0.4731 - categorical_accuracy: 0.8580 - val_loss: 0.4324 - val_categorical_accuracy: 0.8778\n",
            "Epoch 11/25\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 1.0000000474974514e-05.\n",
            "50/50 [==============================] - 58s 1s/step - loss: 0.4635 - categorical_accuracy: 0.8440 - val_loss: 0.5342 - val_categorical_accuracy: 0.8750\n",
            "Epoch 12/25\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 1.0000000656873453e-05.\n",
            "50/50 [==============================] - 57s 1s/step - loss: 0.4403 - categorical_accuracy: 0.8560 - val_loss: 280.8819 - val_categorical_accuracy: 0.8694\n",
            "Epoch 13/25\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 1.0000000656873453e-05.\n",
            "50/50 [==============================] - 58s 1s/step - loss: 0.4162 - categorical_accuracy: 0.8580 - val_loss: 5.2316 - val_categorical_accuracy: 0.8889\n",
            "Epoch 14/25\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 1.0000000656873453e-05.\n",
            "50/50 [==============================] - 56s 1s/step - loss: 0.4493 - categorical_accuracy: 0.8500 - val_loss: 0.4147 - val_categorical_accuracy: 0.8889\n",
            "Epoch 15/25\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 1.0000000656873453e-05.\n",
            "50/50 [==============================] - 57s 1s/step - loss: 0.4725 - categorical_accuracy: 0.8460 - val_loss: 0.4456 - val_categorical_accuracy: 0.8806\n",
            "Epoch 16/25\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 1.0000000656873453e-06.\n",
            "50/50 [==============================] - 56s 1s/step - loss: 0.4289 - categorical_accuracy: 0.8440 - val_loss: 0.4711 - val_categorical_accuracy: 0.8917\n",
            "Epoch 17/25\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 1.0000001111620804e-06.\n",
            "50/50 [==============================] - 57s 1s/step - loss: 0.4095 - categorical_accuracy: 0.8800 - val_loss: 0.4310 - val_categorical_accuracy: 0.8944\n",
            "Epoch 18/25\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 1.0000001111620804e-06.\n",
            "50/50 [==============================] - 55s 1s/step - loss: 0.4545 - categorical_accuracy: 0.8540 - val_loss: 0.5006 - val_categorical_accuracy: 0.8750\n",
            "Epoch 19/25\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 1.0000001111620804e-06.\n",
            "50/50 [==============================] - 57s 1s/step - loss: 0.4902 - categorical_accuracy: 0.8500 - val_loss: 0.4981 - val_categorical_accuracy: 0.8972\n",
            "Epoch 20/25\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 1.0000001111620804e-06.\n",
            "50/50 [==============================] - 56s 1s/step - loss: 0.5478 - categorical_accuracy: 0.8140 - val_loss: 0.4999 - val_categorical_accuracy: 0.8917\n",
            "Epoch 21/25\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 1.0000001111620805e-07.\n",
            "50/50 [==============================] - 54s 1s/step - loss: 0.4120 - categorical_accuracy: 0.8600 - val_loss: 0.8963 - val_categorical_accuracy: 0.8861\n",
            "Epoch 22/25\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 1.000000082740371e-07.\n",
            "50/50 [==============================] - 56s 1s/step - loss: 0.5105 - categorical_accuracy: 0.8160 - val_loss: 0.5276 - val_categorical_accuracy: 0.8972\n",
            "Epoch 23/25\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 1.000000082740371e-07.\n",
            "50/50 [==============================] - 55s 1s/step - loss: 0.4204 - categorical_accuracy: 0.8620 - val_loss: 0.4207 - val_categorical_accuracy: 0.8861\n",
            "Epoch 24/25\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 1.000000082740371e-07.\n",
            "50/50 [==============================] - 56s 1s/step - loss: 0.4492 - categorical_accuracy: 0.8620 - val_loss: 0.4807 - val_categorical_accuracy: 0.8806\n",
            "Epoch 25/25\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 1.000000082740371e-07.\n",
            "50/50 [==============================] - 56s 1s/step - loss: 0.3884 - categorical_accuracy: 0.8720 - val_loss: 0.6923 - val_categorical_accuracy: 0.9111\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5ce4bcfa20>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3AN6A6r-mH6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# validate model\n",
        "generator_test = ModelNetProvider(test_samples, batch_size=1, n_classes=4, sample_size=len(test_samples))\n",
        "\n",
        "val_df = pd.DataFrame({\"prediction\": [], \"ground_truth\": []})\n",
        "for i in range(len(generator_test)):\n",
        "    X, y = generator_test[i]\n",
        "    prediction = new_model.predict(X)\n",
        "    val_df = val_df.append({\"prediction\": l_idx[np.argmax(prediction)],  \"ground_truth\": l_idx[np.argmax(y)]}, \n",
        "                  ignore_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rh58mlMo-mH8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        },
        "outputId": "c9aed50e-d3f1-432a-bf84-8500f1c6d56b"
      },
      "source": [
        "from sklearn import metrics\n",
        "import seaborn as sns\n",
        "from ipywidgets import interact\n",
        "import matplotlib.pyplot as plt\n",
        "sns.set_context('talk')\n",
        "palette = sns.color_palette(n_colors=6)\n",
        "\n",
        "cm = metrics.confusion_matrix(val_df.ground_truth, val_df.prediction)\n",
        "confusionMatrix = pd.DataFrame(cm, index=l_idx, columns=l_idx)\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "sns.heatmap(confusionMatrix, vmin=0, annot=True, fmt=\"d\", linewidth=.5, mask=(confusionMatrix==0), cmap=\"viridis\")\n",
        "plt.xlabel(\"Prediction\")\n",
        "plt.ylabel(\"Truth\")\n",
        "plt.ylim(4, 0)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlQAAAH1CAYAAAAwMGD/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 720x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAToWvNlZmBw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}