{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Multilayer-Perceptron\n",
    "\n",
    "Implementation of a Multilayer Perceptron Network for the classification of handwritten digits.\n",
    "This example is using the MNIST database of handwritten digits\n",
    "(http://yann.lecun.com/exdb/mnist/).\n",
    "\n",
    "The notebook already provides the required functionality to download the dataset. Your task is to implement the missing steps in the training process, test various configurations and finally train a MLP to achieve a high test accuracy.\n",
    "\n",
    "Tasks:\n",
    "- implement the TODOs\n",
    "- run the script so that training starts\n",
    "- try overfitting a fixed set of images\n",
    "- test different network architectures and parameters\n",
    "    - number of hidden layers\n",
    "    - number of neurons\n",
    "    - different optimizers\n",
    "    - learning rate\n",
    "    - adding dropout layer\n",
    "    - normalize data\n",
    "- achieve high test accuracy\n",
    "\n",
    "Help:\n",
    "- use the TensorFlow API Documentation [https://www.tensorflow.org/api_docs/](https://www.tensorflow.org/api_docs/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports MNIST data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "5 0 4 1 9 \n",
      "\n",
      "2 1 3 1 4 \n",
      "\n",
      "3 5 3 6 1 "
     ]
    }
   ],
   "source": [
    "# print first 16 labels\n",
    "for i in range(15):\n",
    "    if i % 5 == 0:\n",
    "        print(\"\\n\")\n",
    "    print(y_train[i], end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots the first 16 entries in the train set\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(15):\n",
    "    plt.subplot(3, 5, i + 1)\n",
    "    plt.imshow(x_train[i], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape[1] * x_train.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define input and output network parameters\n",
    "n_input = x_train.shape[1] * x_train.shape[2] # MNIST data input\n",
    "n_classes =  10 # MNIST total classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: reshape images\n",
    "x_train = np.asarray([np.reshape(x, n_input) for x in x_train])\n",
    "x_test = np.asarray([np.reshape(x, n_input) for x in x_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding of labels\n",
    "def one_hot_encode(a, length):\n",
    "    temp = np.zeros((a.shape[0], length))\n",
    "    temp[np.arange(a.shape[0]), a] = 1\n",
    "    return temp\n",
    "\n",
    "y_train = one_hot_encode(y_train, n_classes)\n",
    "y_test = one_hot_encode(y_test, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define hyper parameters\n",
    "learning_rate = 0.01\n",
    "training_iters = 10\n",
    "batch_size = 60\n",
    "display_step = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholder\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "input = tf.reshape(x, shape=[-1, 28, 28, 1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-11-a06a7cc7e2e5>:2: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From <ipython-input-11-a06a7cc7e2e5>:5: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# MLP definition\n",
    "flatten = tf.layers.flatten(input) # input layer\n",
    "\n",
    "# TODO: define hidden layers\n",
    "hiddenLayer1 = tf.layers.dense(flatten, 500, activation=\"relu\")\n",
    "droptout1 = tf.nn.dropout(hiddenLayer1, rate=0.1)\n",
    "\n",
    "hiddenLayer2 = tf.layers.dense(droptout1, 200, activation=\"relu\")\n",
    "droptout2 = tf.nn.dropout(hiddenLayer2, rate=0.1)\n",
    "\n",
    "hiddenLayer3 = tf.layers.dense(droptout2, 100, activation=\"relu\")\n",
    "droptout3 = tf.nn.dropout(hiddenLayer3, rate=0.1)\n",
    "\n",
    "pred = tf.layers.dense(droptout3, 10, activation=tf.nn.softmax) # output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/ops/losses/losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "# define cost\n",
    "cost = tf.reduce_mean(tf.losses.softmax_cross_entropy(y, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 6000, Minibatch Loss= 2.147833, Training Accuracy= 0.31667\n",
      "Iter 12000, Minibatch Loss= 2.110157, Training Accuracy= 0.35000\n",
      "Iter 18000, Minibatch Loss= 2.110500, Training Accuracy= 0.35000\n",
      "Iter 24000, Minibatch Loss= 2.146201, Training Accuracy= 0.31667\n",
      "Iter 30000, Minibatch Loss= 2.044897, Training Accuracy= 0.41667\n",
      "Iter 36000, Minibatch Loss= 2.044426, Training Accuracy= 0.41667\n",
      "Iter 42000, Minibatch Loss= 1.974553, Training Accuracy= 0.48333\n",
      "Iter 48000, Minibatch Loss= 1.923379, Training Accuracy= 0.53333\n",
      "Iter 54000, Minibatch Loss= 1.944123, Training Accuracy= 0.51667\n",
      "Iter 60000, Minibatch Loss= 1.897766, Training Accuracy= 0.55932\n",
      "Epoch 0 finished\n",
      "Iter 6000, Minibatch Loss= 1.834887, Training Accuracy= 0.63333\n",
      "Iter 12000, Minibatch Loss= 1.859162, Training Accuracy= 0.60000\n",
      "Iter 18000, Minibatch Loss= 1.827816, Training Accuracy= 0.63333\n",
      "Iter 24000, Minibatch Loss= 1.827816, Training Accuracy= 0.63333\n",
      "Iter 30000, Minibatch Loss= 1.861545, Training Accuracy= 0.60000\n",
      "Iter 36000, Minibatch Loss= 1.894485, Training Accuracy= 0.56667\n",
      "Iter 42000, Minibatch Loss= 1.827794, Training Accuracy= 0.63333\n",
      "Iter 48000, Minibatch Loss= 1.910897, Training Accuracy= 0.55000\n",
      "Iter 54000, Minibatch Loss= 1.944158, Training Accuracy= 0.51667\n",
      "Iter 60000, Minibatch Loss= 1.816368, Training Accuracy= 0.64407\n",
      "Epoch 1 finished\n",
      "Iter 6000, Minibatch Loss= 1.744478, Training Accuracy= 0.71667\n",
      "Iter 12000, Minibatch Loss= 1.752831, Training Accuracy= 0.70000\n",
      "Iter 18000, Minibatch Loss= 1.794191, Training Accuracy= 0.66667\n",
      "Iter 24000, Minibatch Loss= 1.787212, Training Accuracy= 0.66667\n",
      "Iter 30000, Minibatch Loss= 1.761162, Training Accuracy= 0.70000\n",
      "Iter 36000, Minibatch Loss= 1.811089, Training Accuracy= 0.65000\n",
      "Iter 42000, Minibatch Loss= 1.744358, Training Accuracy= 0.71667\n",
      "Iter 48000, Minibatch Loss= 1.827816, Training Accuracy= 0.63333\n",
      "Iter 54000, Minibatch Loss= 1.794490, Training Accuracy= 0.66667\n",
      "Iter 60000, Minibatch Loss= 1.816507, Training Accuracy= 0.64407\n",
      "Epoch 2 finished\n",
      "Iter 6000, Minibatch Loss= 1.661156, Training Accuracy= 0.80000\n",
      "Iter 12000, Minibatch Loss= 1.727804, Training Accuracy= 0.73333\n",
      "Iter 18000, Minibatch Loss= 1.677817, Training Accuracy= 0.78333\n",
      "Iter 24000, Minibatch Loss= 1.694451, Training Accuracy= 0.76667\n",
      "Iter 30000, Minibatch Loss= 1.682837, Training Accuracy= 0.78333\n",
      "Iter 36000, Minibatch Loss= 1.744483, Training Accuracy= 0.71667\n",
      "Iter 42000, Minibatch Loss= 1.647515, Training Accuracy= 0.81667\n",
      "Iter 48000, Minibatch Loss= 1.754955, Training Accuracy= 0.70000\n",
      "Iter 54000, Minibatch Loss= 1.743042, Training Accuracy= 0.71667\n",
      "Iter 60000, Minibatch Loss= 1.648337, Training Accuracy= 0.81356\n",
      "Epoch 3 finished\n",
      "Iter 6000, Minibatch Loss= 1.594178, Training Accuracy= 0.86667\n",
      "Iter 12000, Minibatch Loss= 1.562057, Training Accuracy= 0.90000\n",
      "Iter 18000, Minibatch Loss= 1.644490, Training Accuracy= 0.81667\n",
      "Iter 24000, Minibatch Loss= 1.694483, Training Accuracy= 0.76667\n",
      "Iter 30000, Minibatch Loss= 1.710536, Training Accuracy= 0.75000\n",
      "Iter 36000, Minibatch Loss= 1.781454, Training Accuracy= 0.68333\n",
      "Iter 42000, Minibatch Loss= 1.694443, Training Accuracy= 0.76667\n",
      "Iter 48000, Minibatch Loss= 1.711150, Training Accuracy= 0.75000\n",
      "Iter 54000, Minibatch Loss= 1.669434, Training Accuracy= 0.78333\n",
      "Iter 60000, Minibatch Loss= 1.629945, Training Accuracy= 0.83051\n",
      "Epoch 4 finished\n",
      "Iter 6000, Minibatch Loss= 1.623325, Training Accuracy= 0.83333\n",
      "Iter 12000, Minibatch Loss= 1.611482, Training Accuracy= 0.85000\n",
      "Iter 18000, Minibatch Loss= 1.627401, Training Accuracy= 0.83333\n",
      "Iter 24000, Minibatch Loss= 1.677758, Training Accuracy= 0.78333\n",
      "Iter 30000, Minibatch Loss= 1.711149, Training Accuracy= 0.75000\n",
      "Iter 36000, Minibatch Loss= 1.728598, Training Accuracy= 0.73333\n",
      "Iter 42000, Minibatch Loss= 1.662578, Training Accuracy= 0.80000\n",
      "Iter 48000, Minibatch Loss= 1.710920, Training Accuracy= 0.75000\n",
      "Iter 54000, Minibatch Loss= 1.744455, Training Accuracy= 0.71667\n",
      "Iter 60000, Minibatch Loss= 1.646756, Training Accuracy= 0.81356\n",
      "Epoch 5 finished\n",
      "Iter 6000, Minibatch Loss= 1.561150, Training Accuracy= 0.90000\n",
      "Iter 12000, Minibatch Loss= 1.627817, Training Accuracy= 0.83333\n",
      "Iter 18000, Minibatch Loss= 1.611034, Training Accuracy= 0.85000\n",
      "Iter 24000, Minibatch Loss= 1.711024, Training Accuracy= 0.75000\n",
      "Iter 30000, Minibatch Loss= 1.661150, Training Accuracy= 0.80000\n",
      "Iter 36000, Minibatch Loss= 1.760579, Training Accuracy= 0.70000\n",
      "Iter 42000, Minibatch Loss= 1.681766, Training Accuracy= 0.78333\n",
      "Iter 48000, Minibatch Loss= 1.674055, Training Accuracy= 0.78333\n",
      "Iter 54000, Minibatch Loss= 1.646594, Training Accuracy= 0.81667\n",
      "Iter 60000, Minibatch Loss= 1.633087, Training Accuracy= 0.83051\n",
      "Epoch 6 finished\n",
      "Iter 6000, Minibatch Loss= 1.577852, Training Accuracy= 0.88333\n",
      "Iter 12000, Minibatch Loss= 1.627264, Training Accuracy= 0.83333\n",
      "Iter 18000, Minibatch Loss= 1.656518, Training Accuracy= 0.80000\n",
      "Iter 24000, Minibatch Loss= 1.694483, Training Accuracy= 0.76667\n",
      "Iter 30000, Minibatch Loss= 1.694721, Training Accuracy= 0.76667\n",
      "Iter 36000, Minibatch Loss= 1.711151, Training Accuracy= 0.75000\n",
      "Iter 42000, Minibatch Loss= 1.643886, Training Accuracy= 0.81667\n",
      "Iter 48000, Minibatch Loss= 1.710543, Training Accuracy= 0.75000\n",
      "Iter 54000, Minibatch Loss= 1.727737, Training Accuracy= 0.73333\n",
      "Iter 60000, Minibatch Loss= 1.630279, Training Accuracy= 0.83051\n",
      "Epoch 7 finished\n",
      "Iter 6000, Minibatch Loss= 1.563745, Training Accuracy= 0.90000\n",
      "Iter 12000, Minibatch Loss= 1.594483, Training Accuracy= 0.86667\n",
      "Iter 18000, Minibatch Loss= 1.627802, Training Accuracy= 0.83333\n",
      "Iter 24000, Minibatch Loss= 1.627370, Training Accuracy= 0.83333\n",
      "Iter 30000, Minibatch Loss= 1.655081, Training Accuracy= 0.80000\n",
      "Iter 36000, Minibatch Loss= 1.677817, Training Accuracy= 0.78333\n",
      "Iter 42000, Minibatch Loss= 1.611334, Training Accuracy= 0.85000\n",
      "Iter 48000, Minibatch Loss= 1.710979, Training Accuracy= 0.75000\n",
      "Iter 54000, Minibatch Loss= 1.665049, Training Accuracy= 0.80000\n",
      "Iter 60000, Minibatch Loss= 1.613693, Training Accuracy= 0.84746\n",
      "Epoch 8 finished\n",
      "Iter 6000, Minibatch Loss= 1.544483, Training Accuracy= 0.91667\n",
      "Iter 12000, Minibatch Loss= 1.594483, Training Accuracy= 0.86667\n",
      "Iter 18000, Minibatch Loss= 1.625646, Training Accuracy= 0.83333\n",
      "Iter 24000, Minibatch Loss= 1.627344, Training Accuracy= 0.83333\n",
      "Iter 30000, Minibatch Loss= 1.644480, Training Accuracy= 0.81667\n",
      "Iter 36000, Minibatch Loss= 1.742755, Training Accuracy= 0.71667\n",
      "Iter 42000, Minibatch Loss= 1.677500, Training Accuracy= 0.78333\n",
      "Iter 48000, Minibatch Loss= 1.661137, Training Accuracy= 0.80000\n",
      "Iter 54000, Minibatch Loss= 1.644024, Training Accuracy= 0.81667\n",
      "Iter 60000, Minibatch Loss= 1.630642, Training Accuracy= 0.83051\n",
      "Epoch 9 finished\n",
      "Optimization Finished!\n",
      "Test Loss= 1.618835, Test Accuracy= 0.84210\n"
     ]
    }
   ],
   "source": [
    "# initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    epoch = 0\n",
    "    \n",
    "    while epoch < training_iters:\n",
    "\n",
    "        step = 1\n",
    "        begin_pointer = 0\n",
    "\n",
    "        # TODO: define training loop condition\n",
    "        while step <= x_train.shape[0] / batch_size:\n",
    "            # TODO: get batch of images and labels\n",
    "            batch_x = x_train[begin_pointer:(batch_size * step) + 1]\n",
    "            batch_y = y_train[begin_pointer:(batch_size * step) + 1]\n",
    "            begin_pointer = (batch_size * step) + 1\n",
    "            # run optimization op (backprop)\n",
    "            sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "            if step % display_step == 0:\n",
    "                # calculate batch loss and accuracy\n",
    "                loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x,\n",
    "                                                                  y: batch_y})\n",
    "                print (\"Iter \" + str(step*batch_size) + \\\n",
    "                       \", Minibatch Loss= \" + \"{:.6f}\".format(loss) + \\\n",
    "                       \", Training Accuracy= \" + \"{:.5f}\".format(acc))\n",
    "            step += 1\n",
    "\n",
    "        print (\"Epoch {} finished\".format(epoch))\n",
    "        epoch += 1\n",
    "    print (\"Optimization Finished!\")\n",
    "\n",
    "    # TODO: calculate accuracy for test set\n",
    "    #predictions = \n",
    "    #tf.metrics.accuracy(y_train, predictions)\n",
    "    loss, acc = sess.run([cost, accuracy], feed_dict={x: x_test, y: y_test})\n",
    "    print (\"Test Loss= \" + \"{:.6f}\".format(loss) + \", Test Accuracy= \" + \"{:.5f}\".format(acc))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 Hidden Layers (500, 200)\n",
    "activation = ReLu\n",
    "\n",
    "learning_rate = 0.01\n",
    "training_iters = 10\n",
    "batch_size = 60\n",
    "display_step = 100\n",
    "\n",
    "Test Loss= 1.697183, Test Accuracy= 0.76380\n",
    "\n",
    "########################################################################################################\n",
    "\n",
    "3 Hidden Layers (500, 200, 100)\n",
    "activation = ReLu\n",
    "\n",
    "learning_rate = 0.01\n",
    "training_iters = 10\n",
    "batch_size = 60\n",
    "display_step = 100\n",
    "\n",
    "Test Loss= 1.628898, Test Accuracy= 0.83220\n",
    "\n",
    "########################################################################################################\n",
    "\n",
    "3 Hidden Layers (500, 200, 100)\n",
    "1 Dropout 10% (auf den letzten Hiddenlayer)\n",
    "activation = ReLu\n",
    "\n",
    "learning_rate = 0.01\n",
    "training_iters = 10\n",
    "batch_size = 60\n",
    "display_step = 100\n",
    "\n",
    "Test Loss= 1.549171, Test Accuracy= 0.91190\n",
    "\n",
    "########################################################################################################\n",
    "\n",
    "3 Hidden Layers (500, 200, 100)\n",
    "3 Dropout 10% (auf jeden Hiddenlayer)\n",
    "activation = ReLu\n",
    "\n",
    "learning_rate = 0.01\n",
    "training_iters = 10\n",
    "batch_size = 60\n",
    "display_step = 100\n",
    "\n",
    "Test Loss= 1.647166, Test Accuracy= 0.81370\n",
    "\n",
    "########################################################################################################\n",
    "\n",
    "3 Hidden Layers (500, 200, 100)\n",
    "1 Dropout 10% (auf den letzten Hiddenlayer)\n",
    "activation = ReLu\n",
    "\n",
    "learning_rate = 0.01\n",
    "training_iters = 40\n",
    "batch_size = 60\n",
    "display_step = 100\n",
    "\n",
    "Test Loss= 1.601624, Test Accuracy= 0.85900\n",
    "\n",
    "########################################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
