{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atari mit DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"atari_pong.gif\" alt=\"Atari Pong\" height=\"75%\"/>\n",
    "\n",
    "## Vorverarbeitung für Atari Environments\n",
    "Im Gegensatz zur Aufgabe 3 besteht die Observation in dieser Aufgabe aus gerenderten Pixeldaten aus dem Atari Simulator. Um diese sinnvoll und effizient für das Training der Q-Funktion im Neuronalen Netz verwenden zu können, wird eine Reihe von Vorverarbeitungsschritten ausgeführt. Zusätzlich zu diesen gibt es weitere Vorverarbeitungsschritte, um mit Eigenheiten der Atari Environments umzugehen. Diese Schritte sind nachfolgend genauer erklärt und bereits in den Methoden `make_atari` und `wrap_deepmind` als `gym.Wraper` um das eigentliche Environment implementiert.\n",
    "### No-ops after Reset\n",
    "Die ersten 30 Frames pro Episode werden übersprungen. Diese bieten keinen Informationsgehalt, da z.B. bei Pong der Ball sich zu Beginn der Episode zuerst in Richtung Gegner bewegt. Somit ist die Aktionswahl des Agenten in diesem Zeitfenster irrelevant. Für den Agenten beginnt das Spiel mit dem 31sten Frame.\n",
    "### Max and Skip\n",
    "Eine vom Agenten gewählte Aktion wird für vier aufeinanderfolgende Frames ausgeführt. Von diesen vier Frames wird das Maximum der Helligkeitswerte der Pixel gebildet und als Observation vom Environment zurückgegeben. Der Grund für diese Vorgehensweise ist ein Darstellungseigenheit bei den Atari Spielen. So kann es z.B. sein, dass bei Pong der Ball von einem Frame auf den nächsten nicht mehr zu sehen ist, da er sich in einem ungünstigen Winkel schräg bewegt und somit nicht mehr vollständig gerendert wird.\n",
    "### Episodic Life\n",
    "Mit jedem Leben das der Agent verliert endet eine Episode, das Zurücksetzen des Environments passiert jedoch nur bei einem echten Game Over. Dadurch können alle Game States weiterhin erreicht werden, trotz des Episodic Tasks auf Basis der einzelnen Leben des Agenten.\n",
    "### Fire Reset\n",
    "Eine Eigenheit bei den Atari Spielen ist es, dass nach dem Zurücksetzen des Environments die `FIRE`-Taste als Aktion getätigt werden muss, um einen Neustart des Spiels zu initialisieren. Diese \"Schwierigkeit\" wurde zugunsten eines schnelleren Trainings entfernt, indem zu Spielbeginn die `FIRE`-Taste automatisch als erste Aktion gedrückt wird.\n",
    "### Warp Frame\n",
    "In dem ursprünglichen Paper von DeepMind wurde vorgeschlagen, das Inputbild auf 84x84 Pixel zu skalieren. Zusätzlich wird das Bild in Graustufen konvertiert. Damit wird der State-Space von 210x160x3 auf 84x84x1 Pixel reduziert und entsprechend das Lernen beschleunigt.\n",
    "### Clip Reward\n",
    "Je nach Atari Spiel können die Rewards unterschiedlich ausfallen. Um diese zu vereinheitlichen wird die Reward Funktion so fixiert, dass bei einer Niederlage -1, bei einem Sieg +1 und ansonsten 0 als Reward vom Environment zurückgegeben wird.\n",
    "### Framestack\n",
    "Damit der Agent Bewegungen im Spiel als solche erkennen kann, wird die Observation erweitert. Statt nur dem aktuellen Frame, werden die letzten vier Frames zurückgegeben.\n",
    "\n",
    "## Model Structure\n",
    "Dem Netzwerk wird ein State $s$ des Environments übergeben und das Netzwerk gibt wiederum den Q-Wert für jede Aktion, die aus diesem Zustand heraus durchgeführt werden können, aus. Implementierungsdetail (**nicht in Abbildung dargestellt**): Die _action mask_ bildet einen weiteren Input für das Netzwerk (dient allerdings nur der Beschleunigung) und wird mit dem output multiplizert.\n",
    "\n",
    "<img src=\"atari_model_structure.svg\" alt=\"Model Structure\" width=\"100%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 4\n",
    "Löse das PongNoFrameskip-v4 Environment mittels DQN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../setup.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from contextlib import suppress\n",
    "from lib.loggers import TensorBoardLogger, tf_summary_image\n",
    "\n",
    "def interact_with_environment(env, agent, n_episodes=600, max_steps=1000000, train=True, verbose=True):      \n",
    "    statistics = []\n",
    "    tb_logger = TensorBoardLogger(f'./logs/run-{datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")}')\n",
    "    \n",
    "    with suppress(KeyboardInterrupt):\n",
    "        total_step = 0\n",
    "        for episode in range(n_episodes):\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            state = env.reset()\n",
    "            episode_start_time = time.time()\n",
    "            episode_step = 0\n",
    "\n",
    "            while not done:\n",
    "                action = agent.act(state)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "                if train:\n",
    "                    agent.train((state, action, next_state, reward, done))\n",
    "\n",
    "                if episode == 0:\n",
    "                    # for debug purpose log every state of first episode\n",
    "                    for obs in state:\n",
    "                        tb_logger.log_image(f'state_t{episode_step}:', tf_summary_image(np.array(obs, copy=False)),\n",
    "                                            global_step=total_step)\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "                episode_step += 1\n",
    "            \n",
    "            total_step += episode_step\n",
    "\n",
    "            if episode % 10 == 0:\n",
    "                speed = episode_step / (time.time() - episode_start_time)\n",
    "                tb_logger.log_scalar('score', episode_reward, global_step=total_step)\n",
    "                tb_logger.log_scalar('epsilon', agent.epsilon, global_step=total_step)\n",
    "                tb_logger.log_scalar('speed', speed, global_step=total_step)\n",
    "                if verbose:\n",
    "                    print(f'episode: {episode}/{n_episodes}, score: {episode_reward}, steps: {episode_step}, '\n",
    "                          f'total steps: {total_step}, e: {agent.epsilon:.3f}, speed: {speed:.2f} steps/s')\n",
    "\n",
    "            statistics.append({\n",
    "                'episode': episode,\n",
    "                'score': episode_reward,\n",
    "                'steps': episode_step\n",
    "            })\n",
    "                                  \n",
    "            if total_step >= max_steps:\n",
    "                break\n",
    "        \n",
    "    return statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1\n",
    "Implementiere in **agent.py** einen Agenten, der in der Lage ist das Environment zu lösen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from lib.statistics import plot\n",
    "from lib.atari_helpers import wrap_deepmind, make_atari\n",
    "from agent import AtariDQN\n",
    "from tensorflow.keras.backend import set_session\n",
    "from IPython.display import SVG\n",
    "from tensorflow.python.keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "env = make_atari('PongNoFrameskip-v4')\n",
    "env = wrap_deepmind(env, frame_stack=True)\n",
    "\n",
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.shape[0]\n",
    "print(state_size)\n",
    "\n",
    "# Hyperparams\n",
    "annealing_steps = 100000  # not episodes!\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = (epsilon - epsilon_min) / annealing_steps\n",
    "alpha = 0.0001\n",
    "batch_size = 64\n",
    "memory_size = 10000\n",
    "start_replay_step = 10000\n",
    "target_model_update_interval = 1000\n",
    "train_freq = 4\n",
    "\n",
    "agent = AtariDQN(action_size=action_size, state_size=state_size, gamma=gamma, \n",
    "                 epsilon=epsilon, epsilon_decay=epsilon_decay, epsilon_min=epsilon_min, \n",
    "                 alpha=alpha, batch_size=batch_size, memory_size=memory_size,\n",
    "                 start_replay_step=start_replay_step, \n",
    "                 target_model_update_interval=target_model_update_interval, train_freq=train_freq)\n",
    "statistics = interact_with_environment(env, agent, verbose=True)\n",
    "env.close()\n",
    "plot(statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Aufbau Keras Modell**\n",
    "Der Aufbau des Keras-Modells kann zur Verdeutlichung nochmals geplottet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "import tensorflow as tf\n",
    "tf.keras.utils.plot_model(agent.model, to_file='keras_plot_model.png', show_shapes=True)\n",
    "IPython.display.Image('keras_plot_model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Performanceauswertung (Video)**\n",
    "Der folgende Code dient zur Performancebewertung des Agenten. Der (hoffentlich) trainierte Agent wird bei seiner Ausführung gefilmt, trainiert aber nicht weiter. Anschließend wird das Video seiner besten Performance dargestellt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import Monitor\n",
    "from lib.atari_helpers import wrap_deepmind, make_atari\n",
    "from lib.statistics import html_video_embedding\n",
    "\n",
    "# capture every episode and clean 'video' folder before each run\n",
    "env = make_atari('PongNoFrameskip-v4')\n",
    "env = Monitor(env, './video', video_callable=lambda episode_id: True, force=True, uid='id')\n",
    "env = wrap_deepmind(env, frame_stack=True)\n",
    "statistics = interact_with_environment(env, agent, n_episodes=10, train=False, verbose=False)\n",
    "env.close()\n",
    "\n",
    "plot(statistics, y_limits=(-21,21))\n",
    "html_video_embedding(statistics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
